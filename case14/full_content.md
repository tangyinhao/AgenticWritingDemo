# Mamba：利用可选状态空间的线性时间复杂度的序列模型

## 01 研究背景

基础模型，或者说在大量数据上预训练然后为下游任务迁移适应的大模型，在现代机器学习领域已经成为了一种有效的范式。这些基础模型的骨干网络通常都是序列模型，可以在很多样性的领域处理任意输入序列，比如语言、图像、演讲、声音、时序和基因组。虽然这个概念与某种特定的模型结构的选择无关，但是大多数现代基础模型都是基于一种序列模型：Transformer和它核心的注意力层。自注意力机制之所以有效是由于它在一个上下文窗口中密集地传递信息的能力，这允许它对非常复杂的数据进行建模。然而，这种属性带来了根本性的缺点：在有限的窗口外对任何信息建模的无能为力，以及根据窗口长度二次增长的模型规模。很多研究都针对这些问题提出了解决方法，但是都是在牺牲有效性的基础上。并且，所有这些变体都没有在大规模跨领域上被证明是有效的。

最近，结构化状态空间序列模型（SSMs）已经作为一种有希望的时序模型结构出现。这些模型可以被解释为受到经典状态空间模型启发的循环神经网络和卷积神经网络的结合体。这类模型可以达到和循环神经网络或者卷积神经网络一样的计算速度，同时规模以线性或者接近线性的速度增长。而且，这些模型在某些数据领域对长期依赖关系还有原则性的机制，并且有主导的基准测试例如Long Range Arena。许多类型的结构化状态空间序列模型在设计连续信号的领域都取得了成功，但是在例如文本的信息稠密数据上显得不那么有效。

文章提出一种新的选择性状态空间模型，在许多方面对前面的工作进行提升来达到Transformer的建模能力的同时模型以线性复杂度增大。

## 02 方法

### 2.1 选择机制

首先，文章发现之前模型的主要局限性：高效地以一种与输入相关的方式选择数据的能力不足。文章通过依赖输入来参数化SSM模型的参数来设计了一种简单的选择机制，这允许模型过滤掉不相关的信息并且无限期地记住相关的信息。

### 2.2 硬件可感知的算法

上文提到的选择机制这一简单的改变引来了关于模型计算的挑战，实际上，为了计算的高效，所有之前的SSMs都必须是是不变和输入不变的。文章使用一种硬件可感知的算法来克服这一点。这种算法使用扫描替代卷积来循环计算模型，但是不实现扩充了的状态来避免不同层级的GPU内存之间的访问。结果在理论和现代硬件上都比之前的SSMs要快。

### 2.3 结构

文章通过结合之前SSM的设计和Transformer的MLP模块到一个单一的模块来简化之前的深度序列模型结构，得到了一个包含选择性状态空间的简单而且和谐的设计（Mamba）。

首先状态空间模型就是控制人很熟悉的现代控制理论里的东西。文章首先要对连续状态下的状态转移方程做离散化，引入一个delta，计算过程如下：
\[
\overline{A} = \exp(\Delta A) \quad\quad \overline{B} = (\Delta A)^{-1} \left( \exp(\Delta A) - I \right) \cdot \Delta B
\]
离散化系数之后，模型要进行计算。一般来说，在训练阶段为了高效地并行训练采用卷积模式，在推理阶段为了高效地自回归而采用循环模式。

与结构化SSMs一样，选择性SSMs是独立的序列变换，可以灵活的结合到神经网络中去。如图2所示，左边的H3结构式最出名的SSM结构的基础，大体上由受到线性注意力启发的和MLP交错的block组成。文章通过结合这两种成分为一块来简化这种结构，也就是对应的堆叠起来。这种方法是受到图2中间的gated attention unit启发，而这和注意力机制有点相似。

这个结构涉及到通过一种可控的因素E来扩充模型的维度D。对于每个block，大多数的参数都在线性投影层，而少部分在内部的SSM种。SSM的参数对比起来显得少得多。文章重复这个block，并且掺杂标准正则化和残差连接来构成Mamba结构。文章在实验中始终保持E等于2并且使用两次block的堆叠来匹配交错多头注意力层和MLP的Transformer的参数。文章使用SiLU或者Swish激活函数。最后，文章还额外使用一层可选地正则化层，作者选择的是LayerNorm。

## 03 实验

### 3.1 选择性复制任务

选择性复制任务是序列建模中研究最充分的综合任务之一，最初设计用于测试循环模型的记忆能力。如图表3所示，门控的结构例如H3和Mamba只是部分增强了表现，而选择机制（把S4修改成S6）非常轻易地解决了这个任务，特别是和更强力的结构结合在一起的时候。

### 3.2 感应头

感应头是从机械可解释性视角看来的一项简单的任务，却出乎意料地可以对大语言模型的上下文学习能力进行预测。它需要模型进行有联系的重呼和复制，例如，如果模型看到一个范式“齐天大圣 孙悟空”序列，那么下次再遇到“齐天大圣”，模型应该能够从历史中复制出“孙悟空”。

Mamba模型，或者更准确的说是选择性SSM层，可以非常完美的解决这个任务，因为它记忆相关token的同时忽略所有其中的token的能力。它完美的生成了百万级长度的序列，或者是它训练时见过序列长度的4000倍，而同时没有其他的方法能超过2倍。

### 3.3 语言建模

文章在预训练指标和零样本估计上都测试了Mamba结构的标准自回归语言建模能力。文章使用了Pile数据集。

### 3.4 DNA建模

在接下来的DNA实验中，文章研究了模型相对于序列长度的缩放特性。文章只比较了HyenaDNA 和Mamba模型，因为在更长的序列长度下，二次注意力变得非常昂贵。文章在序列长度为1024、4096、16384、65536、262144、1048576上预训练模型。文章固定一个 6 层的模型大小，宽度为 128（约 1.3M-1.4M 参数）。模型被训练了 20K个梯度步长，总共大约330B个tokens。

## 04 总结

长期以来，关于RNN和Transformer之间都有一个难以平衡的天平，就是训练速度和推理速度。对于RNN来说，训练时间相对较长，因为它这种迭代式的结构难以并行化训练，但是在推理时只需要考虑上次状态和本次的输出，这让它的推理速度非常快。而Transformer，虽然在训练时可以非常高效地并行化，但是由于每次输出它都要将之前所有的输入都重新计算一遍，所以推理相对来说比较慢。而本文提出的Mamba结构就旨在打破这种情况，作者发现计算之所以耗时，大部分都浪费在GPU的不同级别之间内存的数据转移，于是设计了硬件感知算法，尽可能让计算都保持在GPU的高速内存之间。同时Mamba的结构使得模型的记忆能力大大加强，各种实验都表面Mamba的效果都要好于之前的方法，有希望击败Transformer成为新的基本模型。然而在论文和源码发出后，大家在跟进的过程中发现Mamba并不像论文说的那样好用，大部分反应速度又慢效果又跑不过Transformer，而且由于Mamba的新结构更底层，模型部署还有环境配置要比别的模型更麻烦，所以建议谨慎看待。