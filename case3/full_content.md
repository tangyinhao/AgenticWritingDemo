# 智能体优化范式

无论是单智能体还是多智能体，其本质都是**在一个给定的交互范式下，通过外部或建模的反馈信号来优化一个或多个智能体的策略**。

**共同核心**: 接受一个“奖励信号”（Reward Signal），然后调整策略网络（LLM 的参数或行为模式），以最大化该信号。它们的“世界观”和“任务来源”是外生的。

**单智能体 (Pillar I)**: 是这个框架下最基础的单元。

- 挑战一在于如何为一个拥有庞大语言空间的模型定义精确的奖励。Search-R1提出搜索引擎与 LLM 推理过程交错融合的 RL 训练范式，通过引入检索 token masking 策略来提升优化稳定性。在设计奖励规则时，直接使用任务结果导向的 EM 作为奖励函数。ReSearch和DeepResearcher采用 word-level F1 估算答案的奖励，同时引入格式遵守惩罚项，即检查模型输出是否包含所有必需的特殊令牌 ``<think>``、``<search>`` 等。除了检查格式正确性，ToolRL还评估了轨迹中工具调用的准确性。具体地，ToolRL 基于 Jaccard 相似度估计工具、参数名称选择的合理性，基于 EM 评估参数值的正确性。为了鼓励工具的有效性（代码的鲁棒性），ARTIST引入了工具执行奖励，即工具调用的成功比例。此外，ARTIST 还定义了状态奖励，旨在激励模型在多轮交互过程中持续维护并更新正确状态（如 Tau-Bench 订单状态、支付方式）。

- 挑战二来自于通信和信用分配的复杂性，但其根本目标依然是响应反馈、优化策略。由于 MoA 的最终目标是高质量地完成用户请求，因此可以将最终任务的奖励信号作为统一的优化依据，广播给流程中的各个模块，从而实现模块间的协同优化。为了准确地进行信用分配，SPA训练了一个进度估计器（progress estimator），它通过累积每一步的贡献，使整个轨迹上的累计奖励与任务的最终奖励相匹配；具体是在 LLM 的最后加了一个轻量级的 MLP，使其能够为每一个状态-动作对（state–action pair）估计一个标量的“贡献分数”。由于信用分配的最终目标是为每个动作/组件估计其优势，而这对大语言模型来说可能比估计预期未来奖励（value model）更简单，因此SWEET-RL提出直接学习每一步动作的优势函数。受 reward model 训练策略启发，SWEET-RL 提出通过轨迹偏好对来训练逐步的优势函数。

**多智能体 (Pillar II)**: 是这个框架的**尺度扩展 (Scaling-up)**。核心问题从“如何优化一个策略”变成了“如何在一组相互影响的策略中找到稳定解”。这个现有工作还没有。

## 单智能体策略

此分支关注优化单个 LLM Agent 的决策流，核心目标是在一个复杂的任务环境中，通过环境反馈迭代提升其策略质量。

- **基于轨迹偏好的优化 (Trajectory-Level Optimization via Preference Modeling)**

    - **核心问题**: 基于人类偏好或者可验证奖励，对 LLM Agent 生成的完整思考链或行动序列（Trajectory）进行整体评估与优化，而非评估单个原子动作（如生成一个词）。

    - **技术路径**: 采用类似 DPO (Direct Preference Optimization)、GRPO (Ghost-Respond Policy Optimization) 等基于偏好对的算法。通过对比两条或多条轨迹的优劣（例如，一条使用 RAG 的轨迹优于一条纯粹幻觉的轨迹），直接在策略空间中进行梯度更新，从而引导模型生成更优的整体解决方案。这有效解决了 LLM 动作空间高维组合性带来的信用分配难题。

- **基于过程的奖励建模 (Process-Supervised Reward Modeling)**

    - **核心问题**: 缓解因任务最终结果（Outcome）稀疏或难以评估而导致的学习效率低下问题。奖励的重点从“最终答案是否正确”转移到“解决问题的过程是否合理”。

    - **技术路径**: 设计并实现一个奖励模型，对智能体决策过程中的关键步骤进行监督。例如，对“正确调用 API”给予正奖励，对“访问与问题无关的网页”给予负奖励，对“相较于自身知识库，更信任权威网络来源”给予启发式奖励。这种稠密的过程反馈（Process Feedback）能更有效地引导智能体学习正确的推理路径。

## 多智能体策略

此分支关注由多个 LLM Agent 组成的系统的行为。随着任务复杂性超越单个智能体的能力上限，多智能体协同成为必然趋势。其核心挑战在于智能体间的**交互**、**通信**与**信用分配**。

- **去中心化独立学习或 MOAR**

    - **描述**: 此为最简化的多智能体模式，其中各智能体独立优化自身策略，并将其他智能体视为环境的一部分。

    - **挑战**: 主要挑战在于环境的“非平稳性”（Non-Stationarity）。由于所有智能体策略同时在演进，导致每个智能体所感知的环境都在动态变化，难以收敛到稳定的纳什均衡。

- **编排式流程的单智能体**

    - **描述**: 当多智能体交互流程固定且可预测时（例如，Agent A 负责搜集资料，Agent B 负责总结，Agent C 负责代码生成），整个系统可以被视为一个单一的、拥有宏观动作（Macro-actions）的决策者。

    - **优势**: 该方法通过将复杂的 MARL 问题简化（Reduce）为单智能体 RL 问题，极大降低了优化难度。可以将整个流程生成的多个版本进行比较，应用第一支柱中的轨迹优化技术。

- **涌现式协同与信用分配**

    - **描述**: 这是 MARL 的真正前沿，适用于需要智能体动态协商、自主分配任务的复杂场景。

    - **核心挑战**:

        - **信用分配 (Credit Assignment)**: 如何将团队的最终成果（正向或负向）公平地分配给每个智能体在过程中的具体贡献。

        - **通信协议 (Communication Protocol)**: 智能体应学习何时、如何以及与谁通信，以最低成本实现信息的高效同步。

    - **前沿方法**: 中心化训练-去中心化执行 (CTDE) 架构，即在训练阶段引入一个全局的“评论家”（Critic）来评估联合动作，而在执行阶段每个智能体仅根据自身观测进行决策。

## 自我进化策略

### 问题合成

复杂任务合成是目前单智能体优化的主流方案。现有的信息查询数据集合成方法通常依赖于在网上自由检索信息，并利用大型语言模型（LLM）从收集到的内容中生成问题。这些方法一般会先将收集的信息组织成结构化格式，然后再使用这些结构化数据作为提示，驱动 LLM 生成自然语言（NL）问题。其核心目标是将信息结构映射为自然语言问题中的推理结构。典型方法如WebDancer和TaskCraft会生成线性的信息链，而其他方法则构建通过网页链接或实体共指网络连接的信息图。然而，这些以信息为导向的方法存在两个关键限制。首先，使用 LLM 进行合成时可能无法完全理解信息结构，导致生成的自然语言问题中推理结构不一致，甚至出现错误答案。其次，信息检索过程杂乱无序，往往带来过多的数据处理开销，并收集到冗余的同质化信息结构，限制了信息结构的多样性，降低了知识覆盖范围。为克服上述限制，WebShaper提出了一种基于形式化驱动的数据集合成范式。该方法首先对信息检索任务进行形式化建模，然后通过该形式化过程系统性地引导数据的生成。在生成过程中，信息的收集由形式化任务需求显式控制。

问题合成主要分为两种类型：

- 前向推理: 基于现有的知识图谱、工具库、网站等，通过路由节点，采样知识图谱中的一个子图，基于 root 和叶节点构造一个问题。随着路由因为叶子节点在变化，所以答案一直在变化。深度通过 root 和根节点的 depth 决定问题的难度。

- 反向推理: 答案是永久不变的，问题在持续迭代。每次迭代会选取当前问题的某一个或者所有实体去做拓展子问题，通过拓展次数保证深度。

### 自我迭代类型

通过复杂任务合成来优化智能体有其局限性，不同定制化任务之间的差异性可能使得模型在适应新任务时变得不灵活，尤其是在训练数据不足的情况下；在现实应用中遭遇快速变化的环境时也会面临系统崩溃的风险。而自我进化的核心，**不是简单地响应反馈，而是创造反馈本身，甚至是创造游戏规则**。它的目标是构建一个能够自我驱动、自我进化的系统，解决数据稀缺和探索无效的根本问题。

不同自我迭代范式的共同核心是创造一个**内生的数据和课程生成循环** (Endogenous Data & Curriculum Generation Loop)，使智能体能够自主地进行有意义的探索和能力提升。聚焦于如何使智能体系统摆脱对人类标注数据的重度依赖，实现自我驱动的能力进化和数据飞轮。其核心在于构建内生的、自动化的学习信号和课程。

- **对抗性自我博弈 (Adversarial Self-Play)**

    - **描述**: 将系统中的智能体赋予对抗性角色，以驱动模型能力的边界拓展和鲁棒性提升。

    - **典型模式**:

        - **出题-解题 (Generator-Solver)**: 一个智能体（Generator）学习生成当前策略难以解决的复杂问题或任务，另一个智能体（Solver）则被迫学习如何解决它们。Self-Challenging提出了一个用于合成大规模多轮工具调用任务的框架。该框架由一个同时扮演 Challenger 和 Executor 角色的 Agent、一个运行 Python 函数的 verifier 构成。Challenger 在未知环境中使用工具探索可能实现的目标，生成具有挑战性的任务。为了保证生成的任务是可行的、可验证的和具有挑战性的，作者提出 Code-as-Task（CaT）表示法，定义任务由如下四个组成部分：自然语言指令（instruction）、可执行验证函数（verification function）、一个正样本 solution 示例和三个负样本 solution 示例。除指令外，其他部分均以代码形式表达，通过代码编译器运行示例自动筛除不合理任务（即要求运行正样本示例后，任务状态可以通过 verifier 验证，而负样本示例则不能通过），提高自动化构造的任务质量。Executor 执行由 Challenger 构造的任务，并基于 verifier 的奖励进行策略更新。AZR框架定义了三类任务类型：演绎推理（Deduction）、溯因推理（Abduction）和归纳推理（Induction），以提升模型在这三种推理范式下的能力。其 self-play 机制在 deduction 任务中的实现逻辑如下：给定任务类型 $a=deduction$ 和 $K$ 个参考样例，proposer 构造一个程序-输入对 $(p,i)$，并在 Python 环境中执行 $p(i)$，获得期望输出 $O_{gold}$。随后，solver 接收 $(p,i)$ 并生成对应的非形式化演绎推理过程及预测结果 $O_{solver}$。系统通过运行环境对比 $O_{solver}$ 与 $O_{gold}$ 的一致性，作为 solver 的奖励 $r_{solve}$，同时将 $1-r_{solve}$ 作为对 proposer 的奖励，从而实现 proposer 与 solver 的对抗协同训练。

        - **攻击-防御 (Red Team-Blue Team)**: 一个智能体（Red Team）学习发现当前系统的漏洞或生成对抗性输入，另一个智能体（Blue Team）则学习如何防御。

- **迭代式自我修正与反思 (Iterative Self-Correction and Reflection)**

    - **描述**: 单个智能体通过生成对自身输出的批判性反思（Critique），并在下一轮迭代中利用该反思来改进其解决方案。这是一种内化的、基于自我对话的学习循环。

    - **机制**: Agent 首先生成一个初始答案，然后启动一个“反思”或“批判”模式来评估该答案的缺陷，最后将“原始问题 + 初始答案 + 批判性反思”作为新的输入，生成一个更优的答案。

- **内生信号进行监督（Intrinsic Signals and Self-Play in Language Model Optimization）**

    - **描述**: 模型在处理难题时表现出较低的置信度，优化置信度应能提升推理能力。

    - **机制**: 利用模型自身的内部置信度度量——称为自我确定性——作为唯一的奖励，而无需外部奖励或标记数据。INTUITOR使用模型输出分布与均匀分布之间的平均 KL 散度作为自我确定性度量。该指标已被证明有助于区分高质量响应与有缺陷的响应。基于这一见解，INTUITOR 通过自我生成的信号指导学习，无需外部监督或手工制作的奖励。INTUITOR 在 in-domain 任务上达到了与监督强化学习相匹配的性能，同时在 OOD 泛化方面表现更优。作者发现内在奖励诱导了涌现的结构化推理和增强的指令跟随能力。

- **自动化课程学习 (Automated Curriculum Learning)**

    - **描述**: 自我博弈和自我修正可以被视为自动化课程学习的特例。更广义上，系统可以学习自动调整任务的难度分布，从易到难，确保智能体始终在“最近发展区”内学习，最大化学习效率。

### 自我迭代原理机制

Self-Play 之所以能持续产生价值，主要源于以下**三种“新信息”的注入**：

- **来自外部环境的真实验证 (Grounding in External Environment)**

    当智能体的行动能够与外部世界交互时（例如调用搜索引擎、代码解释器、API），Self-Play 的循环就被“锚定”在了现实中。

    **例子**: 一个“出题”智能体要求“总结今天关于 AI 的头条新闻”。“解题”智能体必须调用搜索工具。搜索引擎返回的**实时网络内容**就是无法被智能体凭空捏造的“额外信息”。这个外部反馈是客观的、全新的，它为整个系统注入了源源不断的新鲜数据和事实基础，防止了模型自说自话、产生集体幻觉。

- **来自工具执行的隐式反馈 (Implicit Feedback from Tool Execution)**

    即使没有外部世界的实时数据，工具或代码执行环境本身也能提供“新信息”。

    **例子**: 一个智能体负责编写代码，另一个负责写测试用例。当测试用例执行失败时，返回的**错误信息 (Error Message) 和堆栈跟踪 (Stack Trace)** 就是一种宝贵的新信息。这个信息不是来自互联网，而是来自代码解释器这个封闭但确定的环境。它精确地指出了当前策略（生成的代码）的缺陷，为下一次迭代提供了明确的改进方向。

- **来自对抗性探索的认知突破 (Cognitive Breakthrough from Adversarial Exploration)**

    这是最微妙但也是最深刻的一点。即使在信息完全封闭的系统（如棋类游戏），Self-Play 也能创造“新信息”。这种信息是关于**策略本身的认知和理解**。

    **例子**: 在 AlphaGo 的自我对弈中，棋盘的规则是固定的，没有外部信息注入。但一个版本的 AlphaGo 走出的棋（例如“第 37 手”），可能会让另一个版本的 AlphaGo 发现一个之前从未意识到的、深刻的战略漏洞或机会。这个“新信息”是**关于策略空间本身的全新发现**。通过不断地自我挑战，智能体被迫探索其巨大知识库和推理能力的未知组合，从而实现对问题更深层次的理解，这是一种内生的认知突破。

**总结一下**，它之所以强大，正是因为它建立了一个机制，能够通过**外部环境交互、工具执行反馈以及内在的对抗性探索**，持续不断地为学习系统注入“新信息”，从而驱动智能体进行有意义的、超越已有数据边界的进化。