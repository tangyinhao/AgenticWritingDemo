# 冷门的自回归生成模型 ~ 详解 PixelCNN 大家族

图像生成是一个较难建模的任务。为此，我们要用GAN、VAE、Diffusion等精巧的架构来建模图像生成。可是，在NLP中，文本生成却有一种非常简单的实现方法。NLP中有一种基础的概率模型——N元语言模型。N元语言模型可以根据句子的前几个字预测出下一个字的出现概率。比如看到「我爱吃苹……」这句话的前几个字，我们不难猜出下一个字大概率是「果」字。利用N元语言模型，我们可以轻松地实现一个文本生成算法：输入空句子，采样出第一个字；输入第一个字，采样出第二个字；输入前两个字，输出第三个字……以此类推。

既然如此，我们可不可以把相同的方法搬到图像生成里呢？当然可以。虽然图像是二维的数据，不像一维的文本一样有先后顺序，但是我们可以强行给图像的每个像素规定一个顺序。比如，我们可以从左到右，从上到下地给图像标上序号。这样，从逻辑上看，图像也是一个一维数据，可以用NLP中的方法来按照序号实现图像生成了。

PixelCNN就是一个使用这种方法生成图像的模型。可为什么PixelCNN的名气没有GAN、VAE那么大？为什么PixelCNN可以用CNN而不是RNN来处理一维化图像？为什么PixelCNN是一种「自回归模型」？别急，在这篇文章中，我们将认识PixelCNN及其改进模型Gated PixelCNN和PixelCNN++，并认真学习它们的实现代码。看完文章后，这些问题都会迎刃而解。

## PixelCNN

如前所述，PixelCNN借用了NLP里的方法来生成图像。模型会根据前i - 1个像素输出第i个像素的概率分布。训练时，和多分类任务一样，要根据第i个像素的真值和预测的概率分布求交叉熵损失函数；采样时，直接从预测的概率分布里采样出第i个像素。根据这些线索，我们来尝试自己「发明」一遍PixelCNN。

这种模型最朴素的实现方法，是输入一幅图像的前i - 1个像素，输出第i个像素的概率分布，即第i个像素取某种颜色的概率的数组。为了方便讨论，我们先只考虑单通道图像，每个像素的颜色取值只有256种。因此，准确来说，模型的输出是256个经过softmax的概率。这样，我们得到了一个V1.0版本的模型。

等等，模型不是叫「PixelCNN」吗？CNN跑哪去了？的确，对于图像数据，最好还是使用CNN，快捷又有效。因此，我们应该修改模型，令模型的输入为整幅图像和序号i。我们根据序号i，过滤掉i及i之后的像素，用CNN处理图像。输出部分还是保持一致。

改进之后，得到V2.0版本。V2.0版本的模型确实能快速计算第i个像素的概率分布了。可是，CNN是很擅长同时生成一个和原图像长宽相同的张量的，只算一个像素的概率分布还称不上高效。所以，我们可以让模型输入一幅图像，同时输出图像每一处的概率分布。这样得到了V3.0版本。

这次的改进并不能加速采样。但是，在训练时，由于整幅训练图像已知，我们可以在一次前向传播后得到图像每一处的概率分布。假设图像有N个像素，我们就等于是在并行地训练N个样本，训练速度快了N倍！这种并行训练的想法和Transformer如出一辙。

V3.0版本的PixelCNN已经和论文里的PixelCNN非常接近了，我们来探讨一下网络的实现细节。相比普通的CNN，PixelCNN有一个特别的约束：第i个像素只能看到前i-1个像素的信息，不能看到第i个像素及后续像素的信息。对于V2.0版本只要输出一个概率分布的PixelCNN，我们可以通过一些简单处理过滤掉第i个像素之后的信息。而对于并行输出所有概率分布的V3.0版本，让每个像素都忽略后续像素的信息的方法就不是那么显然了。

PixelCNN论文里提出了一种掩码卷积机制，这种机制可以巧妙地掩盖住每个像素右侧和下侧的信息。具体来说，PixelCNN使用了两类掩码卷积，我们把两类掩码卷积分别称为「A类」和「B类」。二者都是对卷积操作的卷积核做了掩码处理，使得卷积核的右下部分不产生贡献。A类和B类的唯一区别在于卷积核的中心像素是否产生贡献。CNN的第一个的卷积层使用A类掩码卷积，之后每一层的都使用B类掩码卷积。

为什么要先用一次A类掩码卷积，再每次使用B类掩码卷积呢？我们不妨来做一个实验。对于一个7x7的图像，我们先用1次3x3 A类掩码卷积，再用若干次3x3 B类掩码卷积。我们观察图像中心处的像素在每次卷积后的感受野（即输入图像中哪些像素的信息能够传递到中心像素上）。

不难看出，经过了第一个A类掩码卷积后，每个像素就已经看不到自己位置上的输入信息了。再经过两次B类卷积，中心像素能够看到左上角大部分像素的信息。这满足PixelCNN的约束。

而如果一直使用A类卷积，每次卷积后中心像素都会看漏一些信息（不妨对比下面这张示意图和上面那张示意图）。多卷几层后，中心像素的值就会和输入图像毫无关系。

只是用B类卷积也是不行的。显然，如果第一层就使用B类卷积，中心像素还是能看到自己位置的输入信息。这打破了PixelCNN的约束。这下，我们能明白为什么只能先用一次A类卷积，再用若干次B类卷积了。

利用两类掩码卷积，PixelCNN满足了每个像素只能接受之前像素的信息这一约束。除此之外，PixelCNN就没有什么特别的地方了。我们可以用任意一种CNN架构来实现PixelCNN。PixelCNN论文使用了一种类似于ResNet的架构。其中，第一个7x7卷积层用了A类掩码卷积，之后所有3x3卷积都是B类掩码卷积。

到目前为止，我们已经成功搭建了处理单通道图像的PixelCNN。现在，我们来尝试把它推广到多通道图像上。相比于单通道图像，多通道图像只不过是一个像素由多个颜色分量组成。我们可以把一个像素的颜色分量看成是子像素。在定义约束关系时，我们规定一个子像素只由它之前的子像素决定。比如对于RGB图像，R子像素由它之前所有像素决定，G子像素由它的R子像素和之前所有像素决定，B子像素由它的R、G子像素和它之前所有像素决定。生成图像时，我们一个子像素一个子像素地生成。

把我们的PixelCNN V3.0推广到RGB图像时，我们要做的第一件事就是修改网络的通道数量。由于现在要预测三个颜色通道，网络的输出应该是一个[256x3, H, W]形状的张量，即每个像素输出三个概率分布，分别表示R、G、B取某种颜色的概率。同时，本质上来讲，网络是在并行地为每个像素计算3组结果。因此，为了达到同样的性能，网络所有的特征图的通道数也要乘3。这里说网络中间的通道数要乘3只是一种方便理解的说法。实际上，中间的通道数可以随意设置，是不是3的倍数都无所谓，只是所有通道在逻辑上被分成了3组。我们稍后会利用到「中间结果的通道数应该能被拆成3组」这一性质。

图像变为多通道后，A类卷积和B类卷积的定义也需要做出一些调整。我们不仅要考虑像素在空间上的约束，还要考虑一个像素内子像素间的约束。为此，我们要用不同的策略实现约束。为了方便描述，我们设卷积核组的形状为[o, i, h, w]，其中o为输出通道数，i为输入通道数，h, w为卷积核的高和宽。

对于通道间的约束，我们要在o, i两个维度上设置掩码。设输出通道可以被拆成三组o1, o2, o3，输入通道可以被拆成三组i1, i2, i3，即o1 = 0:o/3, o2 = o/3:o*2/3, o3 = o*2/3:o，i1 = 0:i/3, i2 = i/3:i*2/3, i3 = i*2/3:i。序号1, 2, 3分别表示这组通道是在维护R, G, B的计算。我们对输入通道组和输出通道组之间进行约束。对于A类卷积，我们令o1看不到i1, i2, i3，o2看不到i2, i3，o3看不到i3；对于B类卷积，我们取消每个通道看不到自己的限制，即在A类卷积的基础上令o1看到i1，o2看到i2，o3看到i3。

对于空间上的约束，我们还是和之前一样，在h, w两个维度上设置掩码。由于「是否看到自己」的处理已经在o, i两个维度里做好了，我们直接在空间上用原来的B类卷积就行。

就这样，修改了通道数，修改了卷积核的掩码后，我们成功实现了论文里的PixelCNN。让我们把这个过程总结一下。PixelCNN的核心思想是给图像的子像素定义一个先后顺序，之后让每个子像素的颜色取值分布由之前所有的子像素决定。实现PixelCNN时，可以用任意一种CNN架构，并注意两点：
1. 网络的输出是一个经softmax的概率分布。
2. 网络的所有卷积层要替换成带掩码的卷积层，第一个卷积层用A类掩码，后面的用B类掩积。

学完了PixelCNN，我们在闲暇之余来谈一谈PixelCNN和其他生成网络的对比情况。精通数学的人，会把图像生成问题看成学习一个图像的分布。每次生成一张图片，就是在图像分布里随机采样一个张量。学习一个分布，最便捷的方法是定义一个带参数$\theta$的概率模型$P_{\theta}$，最大化来自数据集的图像的概率$P_{\theta}(x)$。

可问题来了：一个又方便采样，又能计算概率的模型不好设计。VAE和Diffusion建模了把一个来自正态分布的向量z变化成x的过程，并使用了统计学里的变分推理，求出了$P_{\theta}(x)$的一个下界，再设法优化这个下界。GAN干脆放弃了概率模型，直接拿一个神经网络来评价生成的图像好不好。

PixelCNN则正面挑战了建立概率模型这一任务。它把$P_{\theta}(x)$定义为每个子像素出现概率的乘积，而每个子像素的概率仅由它之前的子像素决定。

由于我们可以轻松地用神经网络建模每个子像素的概率分布并完成采样，PixelCNN的采样也是很方便的。我们可以说PixelCNN是一个既方便采样，又能快速地求出图像概率的模型。

相比与其他生成模型，PixelCNN直接对$P_{\theta}(x)$建模，在和概率相关的指标上表现优秀。很可惜，能最大化数据集的图像的出现概率，并不代表图像的生成质量就很优秀。因此，一直以来，以PixelCNN为代表的对概率直接建模的生成模型没有受到过多的关注。可能只有少数必须要计算图像概率分布的任务才会用到PixelCNN。

除了能直接计算图像的概率外，PixelCNN还有一大特点：PixelCNN能输出离散的颜色值。VAE和GAN这些模型都是把图像的颜色看成一个连续的浮点数，模型的输入和输出的取值范围都位于-1到1之间（有些模型是0到1之间）。而PixelCNN则输出的是像素取某个颜色的概率分布，它能描述的颜色是有限而确定的。假如我们是在生成8位单通道图像，那网络就只输出256个离散的概率分布。能生成离散输出这一特性启发了后续很多生成模型。另外，这一特性也允许我们指定颜色的亮度级别。比如对于黑白手写数字数据集MNIST，我们完全可以用黑、白两种颜色来描述图像，而不是非得用256个灰度级来描述图像。减少亮度级别后，网络的训练速度能快上很多。

在后续的文献中，PixelCNN被归类为了自回归生成模型。这是因为PixelCNN在生成图像时，要先输入空图像，得到第一个像素；把第一个像素填入空图像，输入进模型，得到第二个像素……。也就是说，一个图像被不断扔进模型，不断把上一时刻的输出做为输入。这种用自己之前时刻的状态预测下一个状态的模型，在统计学里被称为自回归模型。如果你在其他图像生成文献中见到了「自回归模型」这个词，它大概率指的就是PixelCNN这种每次生成一个像素，该像素由之前所有像素决定的生成模型。

## Gated PixelCNN

首篇提出PixelCNN的论文叫做Pixel Recurrent Neural Networks。没错！这篇文章的作者提出了一种叫做PixelRNN的架构，PixelCNN只是PixelRNN的一个变种。可能作者一开始也没指望PixelCNN有多强。后来，人们发现PixelCNN的想法还挺有趣的，但是原始的PixelCNN设计得太烂了，于是开始着手改进原始的PixelCNN。

PixelCNN的掩码卷积其实有一个重大漏洞：像素存在视野盲区。

为此，PixelCNN论文的作者们又打了一些补丁，发表了Conditional Image Generation with PixelCNN Decoders这篇论文。这篇论文提出了一种叫做Gated PixelCNN的改进架构。Gated PixelCNN使用了一种更好的掩码卷积机制，消除了原PixelCNN里的视野盲区。Gated PixelCNN使用了两种卷积——垂直卷积和水平卷积——来分别维护一个像素上侧的信息和左侧的信息。垂直卷积的结果只是一些临时量，而水平卷积的结果最终会被网络输出。可以看出，使用这种新的掩码卷积机制后，每个像素能正确地收到之前所有像素的信息了。

除此之外，Gated PixelCNN还把网络中的激活函数从ReLU换成了LSTM的门结构。Gated PixelCNN用新的模块代替了原PixelCNN的普通残差模块。

模块的输入输出都是两个量，左边的量是垂直卷积中间结果，右边的量是最后用来计算输出的量。垂直卷积的结果会经过偏移和一个1x1卷积，再加到水平卷积的结果上。两条计算路线在输出前都会经过门激活单元。所谓门激活单元，就是输入两个形状相同的量，一个做tanh，一个做sigmoid，两个结果相乘再输出。此外，模块右侧那部分还有一个残差连接。

除了上面的两项改动，Gated PixelCNN还做出了其他的一些改动。比如，Gated PixelCNN支持带约束的图像生成，比如根据文字生成图片、根据类别生成图片。用于约束生成的向量会被输入进网络每一层的激活函数中。当然，这些改动不是为了提升原PixelCNN的性能。

## PixelCNN++

之后，VAE的作者也下场了，提出了一种改进版的PixelCNN，叫做PixelCNN++。这篇论文没有多余的废话，在摘要中就简明地指出了PixelCNN++的几项改动：
1. 使用logistic分布代替256路softmax
2. 简化RGB子像素之间的约束关系
3. 使用U-Net架构
4. 使用dropout正则化

这几项改动中，第一项改动是最具启发性的，这一技巧可以拓展到其他任务上。让我们主要学习一下第一项改动，并稍微浏览一下其他几项改动。

### 离散logistic混合似然

原PixelCNN使用256路softmax建模一个像素的颜色概率分布。这么做确实能让模型更加灵活，但有若干缺点。首先，计算这么多的概率值会占很多内存；其次，由于每次训练只有一个位置的标签为1，其他255个位置的标签都是0，模型可学习参数的梯度会很稀疏；最后，在这种概率分布方式下，256种颜色是分开考虑的，这导致模型并不知道相邻的颜色比较相似（比如颜色值128和127、129比较相似）这一事实。总之，用softmax独立地表示各种颜色有着诸多的不足。

作者把颜色的概率分布建模成了连续分布，一下子克服掉了上述所有难题。让我们来仔细看一下新概率分布的定义方法。

首先，新概率分布使用到的连续分布叫做 logistic 分布。它有两个参数：均值 $\mu$ 和方差 $s^2$。它的概率密度函数为：

$$
logistic(\mu, s) = \frac{1}{4s} \, sech^2\!\left(\frac{x - \mu}{2s}\right)
$$

logistic分布的概率密度函数看起来比较复杂。但是，如果对这个函数积分，得到的累计分布函数就是logistic函数。如果令均值为0，方差为1，则logistic函数就是我们熟悉的sigmoid函数了。

接着，每个分布可能是 $K$ 个参数不同的 logistic 分布中的某一个，选择某个 logistic 分布的概率由 $\pi_i$ 表示。比如 $K=2, \pi_1 = 0.3, \pi_2 = 0.7$，就说明有两个可选的 logistic 分布，每个分布有 30% 的概率会使用 1 号 logistic 分布，有 70% 的概率会使用 2 号 logistic 分布。这里的 $\pi_i$ 和原来 256 路 softmax 的输出的意义一样，都是选择某个东西的概率。当然，$K$ 会比 256 要小很多，不然这种改进就起不到减少计算量的作用了。设一个输出颜色为 $v$，它的数学表达式为：

$$
v \sim \sum_{i=1}^{K} \pi_i logistic(\mu_i, s_i)
$$

logistic 分布是一个连续分布，而我们想得到 256 个颜色中的某个颜色的概率，即得到一个离散的分布。因此，在最后一步，我们要从上面这个连续分布里得到一个离散的分布。我们先不管 $K$ 和 $\pi_i$，只考虑只有一个 logistic 分布的情况。根据统计学知识可知，要从连续分布里得到一个离散的分布，可以把定义域拆成若干个区间，对每个区间的概率求积分。在我们的例子里，我们可以把实数集拆成 256 个不同的区间，令 $(-\infty, 0.5]$ 为第 1 个区间，$(0.5, 1.5]$ 为第 2 个区间，……，$(253.5, 254.5]$ 为第 255 个区间，$(254.5, +\infty)$ 为第 256 个区间。


对概率密度函数求积分，就是在累积分布函数上做差。因此，对于某个离散颜色值 $x \in [0, 255], x \in \mathbb{N}$，已知一个 logistic 分布 $logistic(\mu, s)$，则这个颜色值的出现概率是：

$$
P(x \mid \mu, s) =
\begin{cases}
\sigma\!\left(\frac{x + 0.5 - \mu}{s}\right) - \sigma(-\infty), & x = 0 \\
\sigma(\infty) - \sigma\!\left(\frac{x - 0.5 - \mu}{s}\right), & x = 255 \\
\sigma\!\left(\frac{x + 0.5 - \mu}{s}\right) - \sigma\!\left(\frac{x - 0.5 - \mu}{s}\right), & \text{else}
\end{cases}
$$

其中，$\sigma()$ 是 sigmoid 函数。$\sigma\!\left(\frac{x - \mu}{s}\right)$ 就是分布的累积分布函数。


可以看出，使用这种区间划分方法，位于0处和位于255处的颜色的概率相对会高一点。这一特点符合作者统计出的CIFAR-10里的颜色分布规律。

当有 $K$ 个 logistic 分布时，只要把各个分布的概率做一个加权和就行（公式省略掉了 $x$ 位于边界处的情况）。

$$
P(x \mid \pi, \mu, s) = \sum_{i=1}^{K} \pi_i \left[\sigma\!\left(\frac{x + 0.5 - \mu_i}{s_i}\right) - \sigma\!\left(\frac{x - 0.5 - \mu_i}{s_i}\right)\right]
$$

至此，我们已经知道了怎么用一个「离散 logistic 混合似然」来建模颜色的概率分布了。这个更高级的颜色分布以 logistic 分布为基础，以此（概率）$\pi_i$ 混合了 $K$ 个 logistic 分布，并用巧妙的方法把连续分布转换成了离散分布。

## 简化 RGB 子像素之间的约束关系

在原 PixelCNN 中，生成一个像素的 RGB 三个子像素时，为了保证子像素之间的约束，我们要把模型中所有特征图的通道分成三组，并用掩码来维持三组通道间的约束。这样做太麻烦了。因此，PixelCNN+ 对约束做了一定的简化：根据之前所有像素，网络一次性输出三个子像素的均值和方差，而不用掩码区分三个子像素的信息。

当然，只是这样做是不够好的——G 子像素缺少了 R 子像素的信息，B 子像素缺少了 R、G 子像素的信息。为了弥补信息的缺失，PixelCNN 会为每个像素额外输出三个参数 $\alpha, \beta, \gamma$，描述 R 对 G 子像素的约束关系，描述 R 对 B 的约束关系，描述 G 对 B 的约束关系。

让我们采用公式更清晰地描述这一过程。对于某个像素的第 $i$ 个 logistic 分布，网络会输出 10 个参数：

$$
\pi,\ \mu_r,\ \mu_g,\ \mu_b,\ s_r,\ s_g,\ s_b,\ \alpha,\ \beta,\ \gamma
$$

这里，$\pi$ 是选择第 $i$ 个分布的概率，$\mu_r, \mu_g, \mu_b$ 是网络输出的三个子像素的均值，$s_r, s_g, s_b$ 是网络输出的三个子像素的标准差，$\alpha, \beta, \gamma$ 描述子像素之间的约束。

由于缺少了其他子像素的信息，网络直接输出的 $\mu_g, \mu_b$ 是不准的。我们假设子像素之间仅存在简单的线性关系。这样，可以用下面的公式更新 $\mu_g$ 和 $\mu_b$：

$$
\mu_g \leftarrow \mu_g + \alpha \cdot r
$$

$$
\mu_b \leftarrow \mu_b + \beta \cdot r + \gamma \cdot g
$$

更新后的 $\mu_g$ 和 $\mu_b$ 才是训练和采样时使用的最终均值。



你会不会疑惑上面那个公式里的r和g是哪里来的？别忘了，虽然子像素之间的约束被简化了，但是三个子像素还是按先后顺序生成的。在训练时，我们是知道所有子像素的真值的，公式里的r和g来自真值；而在采样时，我们会先用神经网络生成三个子像素的均值和方差，再采样，把采样的r套入公式采样出g，最后把采样的r,g套入公式采样出b。

## 使用U-Net架构

PixelCNN++的网络架构是一个三级U-Net，即网络先下采样两次再上采样两次，同级编码器（下采样部分）的输出会连到解码器（上采样部分）的输入上。这个U-Net和其他任务中的U-Net没什么太大的区别。

## 使用Dropout

过拟合会导致生成图像的观感不好。为此，PixelCNN++采用了Dropout正则化方法，在每个子模块的第一个卷积后面加了一个Dropout。

除了这些改动外，PixelCNN++还使用了类似于Gated PixelCNN里垂直卷积和水平卷积的设计，以消除原PixelCNN里的视野盲区。当然，这点不算做本文的主要贡献。

## 总结

PixelCNN把文本生成的想法套入了图像生成中，令子像素的生成有一个先后顺序。为了在维护先后顺序的同时执行并行训练，PixelCNN使用了掩码卷积。这种并行训练与掩码的设计和Transformer一模一样。如果你理解了Transformer，就能一眼看懂PixelCNN的原理。

相比与其他的图像生成模型，以PixelCNN为代表的自回归模型在生成效果上并不优秀。但是，PixelCNN有两个特点：能准确计算某图像在模型里的出现概率（准确来说在统计学里叫做「似然」）、能生成离散的颜色输出。这些特性为后续诸多工作铺平了道路。

原版的PixelCNN有很多缺陷，后续很多工作对其进行了改进。Gated PixelCNN主要消除了原PixelCNN里的视野盲区，而PixelCNN++提出了一种泛用性很强的用连续分布建模离散颜色值的方法，并用简单的线性约束代替了原先较为复杂的用神经网络表示的子像素之间的约束。

PixelCNN相关的知识难度不高，了解本文介绍的内容足矣。PixelCNN也不是很常见的架构，复现代码的优先级不高，有时间的话阅读一下本文附录中的代码即可。另外，PixelCNN的代码实现里有一个重要的知识点。这个知识点几乎不会在论文和网上的文章里看到，但它对实现是否成功有着重要的影响。如果你对新知识感兴趣，推荐去读一下附录中对其的介绍。